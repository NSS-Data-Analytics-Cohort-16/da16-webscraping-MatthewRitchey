{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7ae8f4ee-3b8b-482f-875e-3a0ce4ac61b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d7c1735d-c573-4b2b-a27c-59b8a3c1e3b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Senior Python Developer\n"
     ]
    }
   ],
   "source": [
    "# 1.\tStart by performing a GET request on the url above and convert the response into a BeautifulSoup object. \n",
    "\n",
    "URL = 'https://realpython.github.io/fake-jobs/'\n",
    "\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "                  \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "                  \"Chrome/122.0.0.0 Safari/537.36\"\n",
    "}\n",
    "\n",
    "response = requests.get(URL, headers=headers)\n",
    "\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "# a. Use the .find method to find the tag containing the first job title (\"Senior Python Developer\"). Hint: can you\n",
    "# find a tag type and/or a class that could be helpful for extracting this information? Extract the text from this\n",
    "# title. \n",
    "first_job_title = soup.find(\"h2\", class_=\"title is-5\")\n",
    "print(first_job_title.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "efcbb00e-689a-485a-a8e2-c7a20d8905f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 100 job titles\n",
      "['Senior Python Developer', 'Energy engineer', 'Legal executive', 'Fitness centre manager', 'Product manager', 'Medical technical officer', 'Physiological scientist', 'Textile designer', 'Television floor manager', 'Waste management officer']\n"
     ]
    }
   ],
   "source": [
    "# b. Now, use what you did for the first title, but extract the job title for all jobs on this page. Store the results\n",
    "# in a list. \n",
    "job_title_tags = soup.find_all(\"h2\", class_=\"title is-5\")\n",
    "\n",
    "# Get the text from each tag\n",
    "job_titles = [tag.text for tag in job_title_tags]\n",
    "\n",
    "print(f\"Found {len(job_titles)} job titles\")\n",
    "print(job_titles[:10])  # preview the first 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9cd258be-4e60-4f89-9a7d-664fff7a7e7b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Company: Payne, Roberts and Davis, Location: Stewartbury, AA, Date: 2021-04-08\n",
      "Company: Vasquez-Davidson, Location: Christopherville, AA, Date: 2021-04-08\n",
      "Company: Jackson, Chambers and Levy, Location: Port Ericaburgh, AA, Date: 2021-04-08\n",
      "Company: Savage-Bradley, Location: East Seanview, AP, Date: 2021-04-08\n",
      "Company: Ramirez Inc, Location: North Jamieview, AP, Date: 2021-04-08\n",
      "Company: Rogers-Yates, Location: Davidville, AP, Date: 2021-04-08\n",
      "Company: Kramer-Klein, Location: South Christopher, AE, Date: 2021-04-08\n",
      "Company: Meyers-Johnson, Location: Port Jonathan, AE, Date: 2021-04-08\n",
      "Company: Hughes-Williams, Location: Osbornetown, AE, Date: 2021-04-08\n",
      "Company: Jones, Williams and Villa, Location: Scotttown, AP, Date: 2021-04-08\n",
      "Company: Garcia PLC, Location: Ericberg, AE, Date: 2021-04-08\n",
      "Company: Gregory and Sons, Location: Ramireztown, AE, Date: 2021-04-08\n",
      "Company: Clark, Garcia and Sosa, Location: Figueroaview, AA, Date: 2021-04-08\n",
      "Company: Bush PLC, Location: Kelseystad, AA, Date: 2021-04-08\n",
      "Company: Salazar-Meyers, Location: Williamsburgh, AE, Date: 2021-04-08\n",
      "Company: Parker, Murphy and Brooks, Location: Mitchellburgh, AE, Date: 2021-04-08\n",
      "Company: Cruz-Brown, Location: West Jessicabury, AA, Date: 2021-04-08\n",
      "Company: Macdonald-Ferguson, Location: Maloneshire, AE, Date: 2021-04-08\n",
      "Company: Williams, Peterson and Rojas, Location: Johnsonton, AA, Date: 2021-04-08\n",
      "Company: Smith and Sons, Location: South Davidtown, AP, Date: 2021-04-08\n",
      "Company: Moss, Duncan and Allen, Location: Port Sara, AE, Date: 2021-04-08\n",
      "Company: Gomez-Carroll, Location: Marktown, AA, Date: 2021-04-08\n",
      "Company: Manning, Welch and Herring, Location: Laurenland, AE, Date: 2021-04-08\n",
      "Company: Lee, Gutierrez and Brown, Location: Lauraton, AP, Date: 2021-04-08\n",
      "Company: Davis, Serrano and Cook, Location: South Tammyberg, AP, Date: 2021-04-08\n",
      "Company: Smith LLC, Location: North Brandonville, AP, Date: 2021-04-08\n",
      "Company: Thomas Group, Location: Port Robertfurt, AA, Date: 2021-04-08\n",
      "Company: Silva-King, Location: Burnettbury, AE, Date: 2021-04-08\n",
      "Company: Pierce-Long, Location: Herbertside, AA, Date: 2021-04-08\n",
      "Company: Walker-Simpson, Location: Christopherport, AP, Date: 2021-04-08\n",
      "Company: Cooper and Sons, Location: West Victor, AE, Date: 2021-04-08\n",
      "Company: Donovan, Gonzalez and Figueroa, Location: Port Aaron, AP, Date: 2021-04-08\n",
      "Company: Morgan, Butler and Bennett, Location: Loribury, AA, Date: 2021-04-08\n",
      "Company: Snyder-Lee, Location: Angelastad, AP, Date: 2021-04-08\n",
      "Company: Harris PLC, Location: Larrytown, AE, Date: 2021-04-08\n",
      "Company: Washington PLC, Location: West Colin, AP, Date: 2021-04-08\n",
      "Company: Brown, Price and Campbell, Location: West Stephanie, AP, Date: 2021-04-08\n",
      "Company: Mcgee PLC, Location: Laurentown, AP, Date: 2021-04-08\n",
      "Company: Dixon Inc, Location: Wrightberg, AP, Date: 2021-04-08\n",
      "Company: Thompson, Sheppard and Ward, Location: Alberttown, AE, Date: 2021-04-08\n",
      "Company: Adams-Brewer, Location: Brockburgh, AE, Date: 2021-04-08\n",
      "Company: Schneider-Brady, Location: North Jason, AE, Date: 2021-04-08\n",
      "Company: Gonzales-Frank, Location: Arnoldhaven, AE, Date: 2021-04-08\n",
      "Company: Smith-Wong, Location: Lake Destiny, AP, Date: 2021-04-08\n",
      "Company: Pierce-Herrera, Location: South Timothyburgh, AP, Date: 2021-04-08\n",
      "Company: Aguilar, Rivera and Quinn, Location: New Jimmyton, AE, Date: 2021-04-08\n",
      "Company: Lowe, Barnes and Thomas, Location: New Lucasbury, AP, Date: 2021-04-08\n",
      "Company: Lewis, Gonzalez and Vasquez, Location: Port Cory, AE, Date: 2021-04-08\n",
      "Company: Taylor PLC, Location: Gileston, AA, Date: 2021-04-08\n",
      "Company: Oliver, Jones and Ramirez, Location: Cindyshire, AA, Date: 2021-04-08\n",
      "Company: Rivera and Sons, Location: East Michaelfort, AA, Date: 2021-04-08\n",
      "Company: Garcia PLC, Location: Joybury, AE, Date: 2021-04-08\n",
      "Company: Johnson, Wells and Kramer, Location: Emmatown, AE, Date: 2021-04-08\n",
      "Company: Gonzalez LLC, Location: Colehaven, AP, Date: 2021-04-08\n",
      "Company: Morgan, White and Macdonald, Location: Port Coryton, AE, Date: 2021-04-08\n",
      "Company: Robinson-Fitzpatrick, Location: Amyborough, AA, Date: 2021-04-08\n",
      "Company: Waters, Wilson and Hoover, Location: Reynoldsville, AA, Date: 2021-04-08\n",
      "Company: Hill LLC, Location: Port Billy, AP, Date: 2021-04-08\n",
      "Company: Li-Gregory, Location: Adamburgh, AA, Date: 2021-04-08\n",
      "Company: Fisher, Ryan and Coleman, Location: Wilsonmouth, AA, Date: 2021-04-08\n",
      "Company: Stewart-Alexander, Location: South Kimberly, AA, Date: 2021-04-08\n",
      "Company: Abbott and Sons, Location: Benjaminland, AP, Date: 2021-04-08\n",
      "Company: Bryant, Santana and Davenport, Location: Zacharyport, AA, Date: 2021-04-08\n",
      "Company: Smith PLC, Location: Port Devonville, AE, Date: 2021-04-08\n",
      "Company: Patterson-Singh, Location: East Thomas, AE, Date: 2021-04-08\n",
      "Company: Martinez-Berry, Location: New Jeffrey, AP, Date: 2021-04-08\n",
      "Company: May, Taylor and Fisher, Location: Davidside, AA, Date: 2021-04-08\n",
      "Company: Bailey, Owen and Thompson, Location: Jamesville, AA, Date: 2021-04-08\n",
      "Company: Vasquez Ltd, Location: New Kelly, AP, Date: 2021-04-08\n",
      "Company: Leblanc LLC, Location: Lake Antonio, AA, Date: 2021-04-08\n",
      "Company: Jackson, Ali and Mckee, Location: New Elizabethside, AA, Date: 2021-04-08\n",
      "Company: Blankenship, Knight and Powell, Location: Millsbury, AE, Date: 2021-04-08\n",
      "Company: Patton, Haynes and Jones, Location: Lloydton, AP, Date: 2021-04-08\n",
      "Company: Wood Inc, Location: Port Jeremy, AA, Date: 2021-04-08\n",
      "Company: Collins Group, Location: New Elizabethtown, AA, Date: 2021-04-08\n",
      "Company: Flores-Nelson, Location: Charlesstad, AE, Date: 2021-04-08\n",
      "Company: Mitchell, Jones and Olson, Location: Josephbury, AE, Date: 2021-04-08\n",
      "Company: Howard Group, Location: Seanfurt, AA, Date: 2021-04-08\n",
      "Company: Kramer-Edwards, Location: Williambury, AA, Date: 2021-04-08\n",
      "Company: Berry-Houston, Location: South Jorgeside, AP, Date: 2021-04-08\n",
      "Company: Mathews Inc, Location: Robertborough, AP, Date: 2021-04-08\n",
      "Company: Riley-Johnson, Location: South Saratown, AP, Date: 2021-04-08\n",
      "Company: Spencer and Sons, Location: Hullview, AA, Date: 2021-04-08\n",
      "Company: Camacho-Sanchez, Location: Philipland, AP, Date: 2021-04-08\n",
      "Company: Oliver and Sons, Location: North Patty, AE, Date: 2021-04-08\n",
      "Company: Eaton PLC, Location: North Stephen, AE, Date: 2021-04-08\n",
      "Company: Stanley-Frederick, Location: Stevensland, AP, Date: 2021-04-08\n",
      "Company: Bradley LLC, Location: Reyesstad, AE, Date: 2021-04-08\n",
      "Company: Parker, Goodwin and Zavala, Location: Bellberg, AP, Date: 2021-04-08\n",
      "Company: Kim-Miles, Location: North Johnland, AE, Date: 2021-04-08\n",
      "Company: Moreno-Rodriguez, Location: Martinezburgh, AE, Date: 2021-04-08\n",
      "Company: Brown-Ortiz, Location: Joshuatown, AE, Date: 2021-04-08\n",
      "Company: Hartman PLC, Location: West Ericstad, AA, Date: 2021-04-08\n",
      "Company: Brooks Inc, Location: Tuckertown, AE, Date: 2021-04-08\n",
      "Company: Washington-Castillo, Location: Perezton, AE, Date: 2021-04-08\n",
      "Company: Nguyen, Yoder and Petty, Location: Lake Abigail, AE, Date: 2021-04-08\n",
      "Company: Holder LLC, Location: Jacobshire, AP, Date: 2021-04-08\n",
      "Company: Yates-Ferguson, Location: Port Susan, AE, Date: 2021-04-08\n",
      "Company: Ortega-Lawrence, Location: North Tiffany, AA, Date: 2021-04-08\n",
      "Company: Fuentes, Walls and Castro, Location: Michelleville, AP, Date: 2021-04-08\n"
     ]
    }
   ],
   "source": [
    "# c. Finally, extract the companies, locations, and posting dates for each job.\n",
    "# Example: first job has Company = \"Payne, Roberts and Davis\",\n",
    "# Location = \"Stewartbury, AA\", Date = \"2021-04-08\"\n",
    "\n",
    "# Find all company tags\n",
    "company_tags = soup.find_all(\"h3\", class_=\"subtitle is-6 company\")\n",
    "companies = [tag.text.strip() for tag in company_tags]\n",
    "\n",
    "# Find all location tags\n",
    "location_tags = soup.find_all(\"p\", class_=\"location\")\n",
    "locations = [tag.text.strip() for tag in location_tags]\n",
    "\n",
    "# Find all posting date tags (no class needed)\n",
    "date_tags = soup.find_all(\"time\")\n",
    "dates = [tag.text.strip() for tag in date_tags]\n",
    "\n",
    "# Make sure we only loop over the shortest list length\n",
    "min_len = min(len(companies), len(locations), len(dates))\n",
    "\n",
    "for i in range(min_len):\n",
    "    print(f\"Company: {companies[i]}, Location: {locations[i]}, Date: {dates[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f9c507e4-a271-469f-9eb4-d9144de59f7a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     Title                     Company              Location  \\\n",
      "0  Senior Python Developer    Payne, Roberts and Davis       Stewartbury, AA   \n",
      "1          Energy engineer            Vasquez-Davidson  Christopherville, AA   \n",
      "2          Legal executive  Jackson, Chambers and Levy   Port Ericaburgh, AA   \n",
      "3   Fitness centre manager              Savage-Bradley     East Seanview, AP   \n",
      "4          Product manager                 Ramirez Inc   North Jamieview, AP   \n",
      "\n",
      "         Date  \n",
      "0  2021-04-08  \n",
      "1  2021-04-08  \n",
      "2  2021-04-08  \n",
      "3  2021-04-08  \n",
      "4  2021-04-08  \n",
      "DataFrame shape: (100, 4)\n"
     ]
    }
   ],
   "source": [
    "# d. Take the lists that you have created and combine them into a pandas DataFrame.\n",
    "jobs_df = pd.DataFrame({\n",
    "    \"Title\": job_titles,\n",
    "    \"Company\": companies,\n",
    "    \"Location\": locations,\n",
    "    \"Date\": dates\n",
    "})\n",
    "\n",
    "print(jobs_df.head())\n",
    "print(f\"DataFrame shape: {jobs_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3a517a16-9259-47da-8bf2-f6999ee9b279",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 200 apply URLs\n",
      "['https://www.realpython.com', 'https://realpython.github.io/fake-jobs/jobs/senior-python-developer-0.html', 'https://www.realpython.com', 'https://realpython.github.io/fake-jobs/jobs/energy-engineer-1.html', 'https://www.realpython.com']\n"
     ]
    }
   ],
   "source": [
    "# 2.\tNext, add a column that contains the url for the \"Apply\" button. Try this in two ways.  \n",
    "\n",
    "# a. First, use the BeautifulSoup find_all method to extract the urls. \n",
    "apply_tags = soup.find_all(\"a\", class_=\"card-footer-item\")\n",
    "apply_urls = [tag[\"href\"].strip() for tag in apply_tags]\n",
    "\n",
    "print(f\"Found {len(apply_urls)} apply URLs\")\n",
    "print(apply_urls[:5])  # preview first 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d352fb94-9807-4046-b3a6-1f3195fb313c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://realpython.github.io/fake-jobs/jobs/senior-python-developer-0.html', 'https://realpython.github.io/fake-jobs/jobs/energy-engineer-1.html', 'https://realpython.github.io/fake-jobs/jobs/legal-executive-2.html', 'https://realpython.github.io/fake-jobs/jobs/fitness-centre-manager-3.html', 'https://realpython.github.io/fake-jobs/jobs/product-manager-4.html']\n",
      "['https://www.realpython.com', 'https://realpython.github.io/fake-jobs/jobs/senior-python-developer-0.html', 'https://www.realpython.com', 'https://realpython.github.io/fake-jobs/jobs/energy-engineer-1.html', 'https://www.realpython.com']\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# b. Next, get those same urls in a different way. Examine the urls and see if you can spot the pattern of how they \n",
    "# are constructed. Then, build the url using the elements you have already extracted. Ensure that the urls that you \n",
    "# created match those that you extracted using BeautifulSoup. Warning: You will need to do some string cleaning and \n",
    "# prep in constructing the urls this way. For example, look carefully at the urls for the \"Software Engineer (Python)\"\n",
    "# job and the \"Scientist, research (maths)\" job.\n",
    "base_url = \"https://realpython.github.io/fake-jobs/jobs/\"\n",
    "\n",
    "# Build slugs from job titles (slugs = URL friendly version of a job title)\n",
    "job_slugs = []\n",
    "for i, title in enumerate(job_titles):\n",
    "    # Lowercase\n",
    "    slug = title.lower()\n",
    "    # Replace spaces with hyphens\n",
    "    slug = slug.replace(\" \", \"-\")\n",
    "    # Remove parentheses, commas, and other punctuation\n",
    "    slug = re.sub(r\"[^\\w\\-]\", \"\", slug)\n",
    "    # Append index\n",
    "    slug = f\"{slug}-{i}.html\"\n",
    "    job_slugs.append(base_url + slug)\n",
    "\n",
    "# Compare with scraped apply_urls\n",
    "print(job_slugs[:5])\n",
    "print(apply_urls[:5])\n",
    "\n",
    "# Verify they match\n",
    "print(job_slugs == apply_urls)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "49894c1e-238f-4091-bca5-00759fb3d63c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fake Jobs for Your Web Scraping Journey Professional asset web application environmentally friendly detail-oriented asset. Coordinate educational dashboard agile employ growth opportunity. Company programs CSS explore role. Html educational grit web application. Oversea SCRUM talented support. Web Application fast-growing communities inclusive programs job CSS. Css discussions growth opportunity explore open-minded oversee. Css Python environmentally friendly collaborate inclusive role. Django no experience oversee dashboard environmentally friendly willing to learn programs. Programs open-minded programs asset. Location: Stewartbury, AA Posted: 2021-04-08\n"
     ]
    }
   ],
   "source": [
    "# 3. Finally, we want to get the job description text for each job. \n",
    "\n",
    "# a. Start by looking at the page for the first job, \n",
    "# https://realpython.github.io/fake-jobs/jobs/senior-python-developer-0.html. Using BeautifulSoup, extract the job\n",
    "# description paragraph.  \n",
    "url = \"https://realpython.github.io/fake-jobs/jobs/senior-python-developer-0.html\"\n",
    "\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0\"\n",
    "}\n",
    "\n",
    "response = requests.get(url, headers=headers)\n",
    "soup_job = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "# Find all paragraphs in the job description section\n",
    "paragraphs = soup_job.find_all(\"p\")\n",
    "description = \" \".join([p.text.strip() for p in paragraphs])\n",
    "\n",
    "print(description)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "56ec47b2-3c4a-4263-9694-de9e49610517",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "                     Title                     Company              Location  \\\n",
      "0  Senior Python Developer    Payne, Roberts and Davis       Stewartbury, AA   \n",
      "1          Energy engineer            Vasquez-Davidson  Christopherville, AA   \n",
      "2          Legal executive  Jackson, Chambers and Levy   Port Ericaburgh, AA   \n",
      "3   Fitness centre manager              Savage-Bradley     East Seanview, AP   \n",
      "4          Product manager                 Ramirez Inc   North Jamieview, AP   \n",
      "\n",
      "         Date                                          Apply_URL  \\\n",
      "0  2021-04-08  https://realpython.github.io/fake-jobs/jobs/se...   \n",
      "1  2021-04-08  https://realpython.github.io/fake-jobs/jobs/en...   \n",
      "2  2021-04-08  https://realpython.github.io/fake-jobs/jobs/le...   \n",
      "3  2021-04-08  https://realpython.github.io/fake-jobs/jobs/fi...   \n",
      "4  2021-04-08  https://realpython.github.io/fake-jobs/jobs/pr...   \n",
      "\n",
      "                                         Description  \n",
      "0  Fake Jobs for Your Web Scraping Journey Profes...  \n",
      "1  Fake Jobs for Your Web Scraping Journey Party ...  \n",
      "2  Fake Jobs for Your Web Scraping Journey Admini...  \n",
      "3  Fake Jobs for Your Web Scraping Journey Tv pro...  \n",
      "4  Fake Jobs for Your Web Scraping Journey Tradit...  \n",
      "0 missing descriptions\n"
     ]
    }
   ],
   "source": [
    "# b. We want to be able to do this for all pages. Write a function which takes as input a url and returns the\n",
    "# description text on that page. For example, if you input\n",
    "# \"https://realpython.github.io/fake-jobs/jobs/television-floor-manager-8.html\" into your function, it should return\n",
    "# the string \"At be than always different American address. Former claim chance prevent why measure too. Almost before\n",
    "# some military outside baby interview. Face top individual win suddenly. Parent do ten after those scientist. Medical\n",
    "# effort assume teacher wall. Significant his himself clearly very. Expert stop area along individual. Three own bank\n",
    "# recognize special good along.\".\n",
    "\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "\n",
    "# Function to fetch job description from a job detail page\n",
    "def get_job_description(url):\n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup_job = BeautifulSoup(response.content, \"html.parser\")\n",
    "    paragraphs = soup_job.find_all(\"p\")\n",
    "    return \" \".join([p.text.strip() for p in paragraphs])\n",
    "\n",
    "# First, scrape only the \"Apply\" button URLs (avoid \"More Info\")\n",
    "apply_tags = soup.find_all(\"a\", class_=\"card-footer-item\", string=\"Apply\")\n",
    "apply_urls = [tag[\"href\"].strip() for tag in apply_tags]\n",
    "\n",
    "print(len(apply_urls))  # should be 100\n",
    "\n",
    "# Add Apply_URL column before using it\n",
    "jobs_df[\"Apply_URL\"] = apply_urls\n",
    "\n",
    "# c. Use the .apply method on the url column you created above to retrieve the description text for all of the jobs.\n",
    "\n",
    "# Now apply the function to each Apply_URL\n",
    "jobs_df[\"Description\"] = jobs_df[\"Apply_URL\"].apply(get_job_description)\n",
    "\n",
    "# Preview the first few rows\n",
    "print(jobs_df.head())\n",
    "\n",
    "# Optional: check if any descriptions are missing\n",
    "print(jobs_df[\"Description\"].isna().sum(), \"missing descriptions\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "44c599f9-cd4e-4b34-a472-ffa256aa459a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Webscraping\n",
    "# In this exercise, you'll practice using BeautifulSoup to parse the content of a web page. The page that you'll be\n",
    "# scraping, https://realpython.github.io/fake-jobs/, contains job listings. Your job is to extract the data on each \n",
    "# job and convert into a pandas DataFrame.\n",
    "\n",
    "# 1.\tStart by performing a GET request on the url above and convert the response into a BeautifulSoup object. \n",
    "\n",
    "# a. Use the .find method to find the tag containing the first job title (\"Senior Python Developer\"). Hint: can you\n",
    "# find a tag type and/or a class that could be helpful for extracting this information? Extract the text from this\n",
    "# title.  \n",
    "\n",
    "# b. Now, use what you did for the first title, but extract the job title for all jobs on this page. Store the results\n",
    "# in a list.  \n",
    "\n",
    "# c. Finally, extract the companies, locations, and posting dates for each job. For example, the first job has a \n",
    "# company of \"Payne, Roberts and Davis\", a location of \"Stewartbury, AA\", and a posting date of \"2021-04-08\". Ensure\n",
    "# that the text that you extract is clean, meaning no extra spaces or other characters at the beginning or end.  \n",
    "\n",
    "# d. Take the lists that you have created and combine them into a pandas DataFrame.\n",
    "\n",
    "# 2.\tNext, add a column that contains the url for the \"Apply\" button. Try this in two ways.  \n",
    "\n",
    "# a. First, use the BeautifulSoup find_all method to extract the urls.  \n",
    "\n",
    "# b. Next, get those same urls in a different way. Examine the urls and see if you can spot the pattern of how they \n",
    "# are constructed. Then, build the url using the elements you have already extracted. Ensure that the urls that you \n",
    "# created match those that you extracted using BeautifulSoup. Warning: You will need to do some string cleaning and \n",
    "# prep in constructing the urls this way. For example, look carefully at the urls for the \"Software Engineer (Python)\"\n",
    "# job and the \"Scientist, research (maths)\" job.\n",
    "\n",
    "# 3.\tFinally, we want to get the job description text for each job.  \n",
    "\n",
    "# a. Start by looking at the page for the first job, \n",
    "# https://realpython.github.io/fake-jobs/jobs/senior-python-developer-0.html. Using BeautifulSoup, extract the job\n",
    "# description paragraph.  \n",
    "\n",
    "# b. We want to be able to do this for all pages. Write a function which takes as input a url and returns the\n",
    "# description text on that page. For example, if you input\n",
    "# \"https://realpython.github.io/fake-jobs/jobs/television-floor-manager-8.html\" into your function, it should return\n",
    "# the string \"At be than always different American address. Former claim chance prevent why measure too. Almost before\n",
    "# some military outside baby interview. Face top individual win suddenly. Parent do ten after those scientist. Medical\n",
    "# effort assume teacher wall. Significant his himself clearly very. Expert stop area along individual. Three own bank\n",
    "# recognize special good along.\".\n",
    "\n",
    "# c. Use the .apply method on the url column you created above to retrieve the description text for all of the jobs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
